{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2335e623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skmultiflow.data import DataStream\n",
    "from skmultiflow.evaluation import EvaluatePrequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./artificial/HyperplaneDataset.csv already exists, skipping generation.\n",
      "./artificial/WaveformDataset.csv already exists, skipping generation.\n"
     ]
    }
   ],
   "source": [
    "from skmultiflow.data import WaveformGenerator, HyperplaneGenerator\n",
    "\n",
    "# Create folders if they don't exist\n",
    "os.makedirs('./artificial', exist_ok=True)\n",
    "os.makedirs('./real-world', exist_ok=True)\n",
    "\n",
    "# === Generate synthetic datasets if not already saved ===\n",
    "def generate_synthetic_dataset(generator, num_samples, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        X, y = [], []\n",
    "        for _ in range(num_samples):\n",
    "            xi, yi = generator.next_sample()\n",
    "            X.append(xi[0])\n",
    "            y.append(yi[0])\n",
    "        df = pd.DataFrame(X)\n",
    "        df['label'] = y\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Saved {filename}\")\n",
    "    else:\n",
    "        print(f\"{filename} already exists, skipping generation.\")\n",
    "\n",
    "# Generate Hyperplane Dataset\n",
    "hyperplane_gen = HyperplaneGenerator(n_features=10,random_state=2002) # Default constructor already produces 2 classes\n",
    "generate_synthetic_dataset(\n",
    "    hyperplane_gen,\n",
    "    num_samples=100000,\n",
    "    filename='./artificial/HyperplaneDataset.csv'\n",
    ")\n",
    "\n",
    "# Generate Waveform Dataset\n",
    "waveform_gen = WaveformGenerator(random_state=2002)\n",
    "generate_synthetic_dataset(\n",
    "    waveform_gen,\n",
    "    num_samples=100000,\n",
    "    filename='./artificial/WaveformDataset.csv'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c18a95db",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootDir = './'\n",
    "\n",
    "artificialDatasets = {}\n",
    "for filename in os.listdir(rootDir + 'artificial'):\n",
    "    if filename.endswith('.csv'):\n",
    "        artificialDatasets[os.path.splitext(filename)[0]] = pd.read_csv(rootDir + 'artificial/' + filename)\n",
    "\n",
    "realWorldDatasets = {}\n",
    "for filename in os.listdir(rootDir + 'real-world'):\n",
    "    if filename.endswith('.csv'):\n",
    "        realWorldDatasets[os.path.splitext(filename)[0]] = pd.read_csv(rootDir + 'real-world/' + filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27d3f5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['HyperplaneDataset', 'WaveformDataset'])\n",
      "dict_keys(['rialto', 'spam'])\n"
     ]
    }
   ],
   "source": [
    "# Display some results\n",
    "print(artificialDatasets.keys())\n",
    "print(realWorldDatasets.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e472d3",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f581ea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skmultiflow as skm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model_awe = skm.meta.AccuracyWeightedEnsembleClassifier()\n",
    "model_arf = skm.meta.AdaptiveRandomForestClassifier()\n",
    "model_dwm = skm.meta.DynamicWeightedMajorityClassifier()\n",
    "model_samKNN = skm.lazy.SAMKNNClassifier()\n",
    "model_lb = skm.meta.LeveragingBaggingClassifier()\n",
    "model_mlp = MLPClassifier(hidden_layer_sizes=(16, 16), random_state=2002)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1413345d",
   "metadata": {},
   "source": [
    "Construct instances of the classification models. For each dataset, use the\n",
    "Interleaved Test-Then-Train approach to train and evaluate the performance\n",
    "of these classifiers, using prediction accuracy as the evaluation metric. Re-\n",
    "port the following results for the classification models on each dataset:\n",
    "• Overall accuracy: The overall prediction accuracy of the models.\n",
    "• Prequential accuracy plot: Prequential accuracy is defined as the pre-\n",
    "diction accuracy of a model over the w most recent data instances. Use\n",
    "20 evaluation windows of size (dataset size/20) to calculate prequen-\n",
    "tial accuracy values. Plot the obtained accuracy values over time for\n",
    "each dataset.\n",
    "Discuss the performance of the classification models. Evaluate how the MLP\n",
    "model performs compared to the ensemble models. Analyze the implications\n",
    "of the observed fluctuations in accuracy values over time in the prequential\n",
    "accuracy plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88c20d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_datasets(datasets_dict, datasets_name, model, model_name):\n",
    "    results = {}\n",
    "\n",
    "    for dataset_name, data in datasets_dict.items():\n",
    "        print(f\"\\nEvaluating on {datasets_name} dataset: {dataset_name}\")\n",
    "\n",
    "        # Prepare data stream\n",
    "        X = data.iloc[:, :-1].values\n",
    "        y = data.iloc[:, -1].values\n",
    "        stream = DataStream(X, y)\n",
    "\n",
    "        if model_name == \"MLP\":\n",
    "            model_instance = model.__class__(**model.get_params())\n",
    "            unique_classes = np.unique(y)\n",
    "            model_instance.partial_fit(X[:1], y[:1], classes=unique_classes)\n",
    "        else:\n",
    "            model_instance = model.__class__()\n",
    "\n",
    "        # Setup evaluator\n",
    "        evaluator = EvaluatePrequential(\n",
    "            show_plot=False,\n",
    "            pretrain_size=0,\n",
    "            max_samples=len(y),\n",
    "            n_wait=len(y)//20,\n",
    "            metrics=['accuracy'],\n",
    "            output_file=f'results_{datasets_name}_{dataset_name}_{model_name}.csv'\n",
    "        )\n",
    "\n",
    "        # Evaluate model\n",
    "        evaluator.evaluate(stream=stream, model=[model_instance], model_names=[model_name])\n",
    "\n",
    "        # Store results\n",
    "        results[dataset_name] = (evaluator, model_name)\n",
    "\n",
    "        # Plot (from CSV or measurements if needed)\n",
    "        plot_prequential_accuracy_from_csv(f'results_{datasets_name}_{dataset_name}_{model_name}.csv', model_name, dataset_name, datasets_name)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_prequential_accuracy_from_csv(csv_file, model_name, dataset_name, datasets_name):\n",
    "    df = pd.read_csv(csv_file, comment='#')\n",
    "\n",
    "    accuracy_values = df['mean_acc_[' + model_name + ']']\n",
    "    sample_counts = df['id']\n",
    "\n",
    "    # Plot the accuracy over time\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(sample_counts, accuracy_values, label=model_name)\n",
    "    plt.title(f'Prequential Accuracy on {dataset_name} ({datasets_name})')\n",
    "    plt.xlabel('Number of samples processed')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'prequential_accuracy_{datasets_name}_{dataset_name}_{model_name}.png')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate and report AWE on real datasets\n",
    "results_awe_real = evaluate_model_on_datasets(realWorldDatasets, \"real-world\", model_awe, \"AWE\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on artificial dataset: HyperplaneDataset\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Evaluating...\n",
      " #################### [100%] [303.98s]\n",
      "Processed samples: 100000\n",
      "Mean performance:\n",
      "AWE - Accuracy     : 0.9199\n",
      "\n",
      "Evaluating on artificial dataset: WaveformDataset\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Evaluating...\n",
      " #################### [100%] [813.20s]\n",
      "Processed samples: 100000\n",
      "Mean performance:\n",
      "AWE - Accuracy     : 0.8190\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and report AWE on artificial datasets\n",
    "results_awe_artificial = evaluate_model_on_datasets(artificialDatasets, \"artificial\", model_awe, \"AWE\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on real-world dataset: rialto\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Evaluating...\n",
      " #################### [100%] [107.12s]\n",
      "Processed samples: 82250\n",
      "Mean performance:\n",
      "SAMKNN - Accuracy     : 0.8136\n",
      "\n",
      "Evaluating on real-world dataset: spam\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Evaluating...\n",
      " #################### [100%] [43.76s]\n",
      "Processed samples: 6213\n",
      "Mean performance:\n",
      "SAMKNN - Accuracy     : 0.9619\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and report SAMKNN on real datasets\n",
    "results_samknn_real = evaluate_model_on_datasets(realWorldDatasets, \"real-world\", model_samKNN, \"SAMKNN\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on artificial dataset: HyperplaneDataset\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Evaluating...\n",
      " #################### [100%] [147.18s]\n",
      "Processed samples: 100000\n",
      "Mean performance:\n",
      "SAMKNN - Accuracy     : 0.8782\n",
      "\n",
      "Evaluating on artificial dataset: WaveformDataset\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Evaluating...\n",
      " #################### [100%] [95.93s]\n",
      "Processed samples: 100000\n",
      "Mean performance:\n",
      "SAMKNN - Accuracy     : 0.8449\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and report SAMKNN on artificial datasets\n",
    "results_samknn_artificial = evaluate_model_on_datasets(artificialDatasets, \"artificial\", model_samKNN, \"SAMKNN\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on real-world dataset: rialto\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Evaluating...\n",
      " #################### [100%] [494.23s]\n",
      "Processed samples: 82250\n",
      "Mean performance:\n",
      "DWM - Accuracy     : 0.3295\n",
      "\n",
      "Evaluating on real-world dataset: spam\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Evaluating...\n",
      " #################### [100%] [432.24s]\n",
      "Processed samples: 6213\n",
      "Mean performance:\n",
      "DWM - Accuracy     : 0.8835\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and report DWM on real datasets\n",
    "results_dwm_real = evaluate_model_on_datasets(realWorldDatasets, \"real-world\", model_dwm, \"DWM\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on artificial dataset: HyperplaneDataset\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Evaluating...\n",
      " #################### [100%] [104.68s]\n",
      "Processed samples: 100000\n",
      "Mean performance:\n",
      "DWM - Accuracy     : 0.9306\n",
      "\n",
      "Evaluating on artificial dataset: WaveformDataset\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Evaluating...\n",
      " #################### [100%] [235.97s]\n",
      "Processed samples: 100000\n",
      "Mean performance:\n",
      "DWM - Accuracy     : 0.7965\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and report DWM on artificial datasets\n",
    "results_dwm_artificial = evaluate_model_on_datasets(artificialDatasets, \"artificial\", model_dwm, \"DWM\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on real-world dataset: rialto\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Evaluating...\n",
      " #################### [100%] [1356.76s]\n",
      "Processed samples: 82250\n",
      "Mean performance:\n",
      "ARF - Accuracy     : 0.7922\n",
      "\n",
      "Evaluating on real-world dataset: spam\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Evaluating...\n",
      " #################### [100%] [167.90s]\n",
      "Processed samples: 6213\n",
      "Mean performance:\n",
      "ARF - Accuracy     : 0.9501\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and report ARF on real datasets\n",
    "results_arf_real = evaluate_model_on_datasets(realWorldDatasets, \"real-world\", model_arf, \"ARF\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on artificial dataset: HyperplaneDataset\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Evaluating...\n",
      " ################---- [80%] [683.34s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yago\\Desktop\\datastream thing\\venv\\lib\\site-packages\\skmultiflow\\drift_detection\\adwin.py:350: RuntimeWarning: overflow encountered in int_scalars\n",
      "  self.mdbl_width += self.width\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " #################### [100%] [877.58s]\n",
      "Processed samples: 100000\n",
      "Mean performance:\n",
      "ARF - Accuracy     : 0.8711\n",
      "\n",
      "Evaluating on artificial dataset: WaveformDataset\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Evaluating...\n",
      " #################### [100%] [1338.46s]\n",
      "Processed samples: 100000\n",
      "Mean performance:\n",
      "ARF - Accuracy     : 0.8337\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and report ARF on artificial datasets\n",
    "results_arf_artificial = evaluate_model_on_datasets(artificialDatasets, \"artificial\", model_arf, \"ARF\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on real-world dataset: rialto\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Evaluating...\n",
      " #################### [100%] [3255.30s]\n",
      "Processed samples: 82250\n",
      "Mean performance:\n",
      "LB - Accuracy     : 0.8463\n",
      "\n",
      "Evaluating on real-world dataset: spam\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Evaluating...\n",
      " #################### [100%] [4054.24s]\n",
      "Processed samples: 6213\n",
      "Mean performance:\n",
      "LB - Accuracy     : 0.9390\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and report LB on real datasets\n",
    "results_lb_real = evaluate_model_on_datasets(realWorldDatasets, \"real-world\", model_lb, \"LB\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on artificial dataset: HyperplaneDataset\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Evaluating...\n",
      " #################### [100%] [2189.38s]\n",
      "Processed samples: 100000\n",
      "Mean performance:\n",
      "LB - Accuracy     : 0.7277\n",
      "\n",
      "Evaluating on artificial dataset: WaveformDataset\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Evaluating...\n",
      " #################### [100%] [3602.60s]\n",
      "Processed samples: 100000\n",
      "Mean performance:\n",
      "LB - Accuracy     : 0.7528\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and report LB on artificial datasets\n",
    "results_lb_artificial = evaluate_model_on_datasets(artificialDatasets, \"artificial\", model_lb, \"LB\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on real-world dataset: rialto\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Evaluating...\n",
      " #################### [100%] [95.62s]\n",
      "Processed samples: 82250\n",
      "Mean performance:\n",
      "MLP - Accuracy     : 0.4977\n",
      "\n",
      "Evaluating on real-world dataset: spam\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Evaluating...\n",
      " #################### [100%] [8.55s]\n",
      "Processed samples: 6213\n",
      "Mean performance:\n",
      "MLP - Accuracy     : 0.9614\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and report MLP on real datasets\n",
    "results_mlp_real = evaluate_model_on_datasets(realWorldDatasets, \"real-world\", model_mlp, \"MLP\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on artificial dataset: HyperplaneDataset\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Evaluating...\n",
      " #################### [100%] [114.57s]\n",
      "Processed samples: 100000\n",
      "Mean performance:\n",
      "MLP - Accuracy     : 0.9264\n",
      "\n",
      "Evaluating on artificial dataset: WaveformDataset\n",
      "Prequential Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Evaluating...\n",
      " #################### [100%] [113.80s]\n",
      "Processed samples: 100000\n",
      "Mean performance:\n",
      "MLP - Accuracy     : 0.8449\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and report MLP on artificial datasets\n",
    "results_mlp_artificial = evaluate_model_on_datasets(artificialDatasets, \"artificial\", model_mlp, \"MLP\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary Reports ===\n",
      "AWE Real World Datasets: {'rialto': 0.41155, 'spam': 0.723966}\n",
      "AWE Artificial Datasets: {'HyperplaneDataset': 0.884, 'WaveformDataset': 0.7906}\n",
      "SAMKNN Real World Datasets: {'rialto': 0.813629, 'spam': 0.9618540000000001}\n",
      "SAMKNN Artificial Datasets: {'HyperplaneDataset': 0.8782, 'WaveformDataset': 0.8448899999999999}\n",
      "DWM Real World Datasets: {'rialto': 0.329495, 'spam': 0.8834700000000001}\n",
      "DWM Artificial Datasets: {'HyperplaneDataset': 0.93056, 'WaveformDataset': 0.79655}\n",
      "ARF Real World Datasets: {'rialto': 0.7921699999999999, 'spam': 0.950105}\n",
      "ARF Artificial Datasets: {'HyperplaneDataset': 0.8710700000000001, 'WaveformDataset': 0.83367}\n",
      "LB Real World Datasets: {'rialto': 0.8462860000000001, 'spam': 0.938999}\n",
      "LB Artificial Datasets: {'HyperplaneDataset': 0.7276600000000001, 'WaveformDataset': 0.7528199999999999}\n",
      "MLP Real World Datasets: {'rialto': 0.49773900000000004, 'spam': 0.9613709999999999}\n",
      "MLP Artificial Datasets: {'HyperplaneDataset': 0.9264100000000001, 'WaveformDataset': 0.84495}\n"
     ]
    }
   ],
   "source": [
    "# Generate summary reports from the .csv files\n",
    "def generate_summary_report(datasets_dict, datasets_name, model_name):\n",
    "    summary = {}\n",
    "\n",
    "    for dataset_name, data in datasets_dict.items():\n",
    "        csv_file = f'results_{datasets_name}_{dataset_name}_{model_name}.csv'\n",
    "        df = pd.read_csv(csv_file, comment='#')\n",
    "\n",
    "        # Overall accuracy\n",
    "        overall_accuracy = df['mean_acc_[' + model_name + ']'].iloc[-1]\n",
    "        summary[dataset_name] = overall_accuracy\n",
    "\n",
    "    return summary\n",
    "\n",
    "summary_awe_real = generate_summary_report(realWorldDatasets, \"real-world\", \"AWE\")\n",
    "summary_awe_artificial = generate_summary_report(artificialDatasets, \"artificial\", \"AWE\")\n",
    "summary_samknn_real = generate_summary_report(realWorldDatasets, \"real-world\", \"SAMKNN\")\n",
    "summary_samknn_artificial = generate_summary_report(artificialDatasets, \"artificial\", \"SAMKNN\")\n",
    "summary_dwm_real = generate_summary_report(realWorldDatasets, \"real-world\", \"DWM\")\n",
    "summary_dwm_artificial = generate_summary_report(artificialDatasets, \"artificial\", \"DWM\")\n",
    "summary_arf_real = generate_summary_report(realWorldDatasets, \"real-world\", \"ARF\")\n",
    "summary_arf_artificial = generate_summary_report(artificialDatasets, \"artificial\", \"ARF\")\n",
    "summary_lb_real = generate_summary_report(realWorldDatasets, \"real-world\", \"LB\")\n",
    "summary_lb_artificial = generate_summary_report(artificialDatasets, \"artificial\", \"LB\")\n",
    "summary_mlp_real = generate_summary_report(realWorldDatasets, \"real-world\", \"MLP\")\n",
    "summary_mlp_artificial = generate_summary_report(artificialDatasets, \"artificial\", \"MLP\")\n",
    "# Print them all\n",
    "print(\"=== Summary Reports ===\")\n",
    "print(\"AWE Real World Datasets:\", summary_awe_real)\n",
    "print(\"AWE Artificial Datasets:\", summary_awe_artificial)\n",
    "print(\"SAMKNN Real World Datasets:\", summary_samknn_real)\n",
    "print(\"SAMKNN Artificial Datasets:\", summary_samknn_artificial)\n",
    "print(\"DWM Real World Datasets:\", summary_dwm_real)\n",
    "print(\"DWM Artificial Datasets:\", summary_dwm_artificial)\n",
    "print(\"ARF Real World Datasets:\", summary_arf_real)\n",
    "print(\"ARF Artificial Datasets:\", summary_arf_artificial)\n",
    "print(\"LB Real World Datasets:\", summary_lb_real)\n",
    "print(\"LB Artificial Datasets:\", summary_lb_artificial)\n",
    "print(\"MLP Real World Datasets:\", summary_mlp_real)\n",
    "print(\"MLP Artificial Datasets:\", summary_mlp_artificial)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Generate a plot that plots the different models together on the same plot, for a single dataset, to compare\n",
    "def plot_comparison(datasets_dict, datasets_name, model_names, dataset_name):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    for model_name in model_names:\n",
    "        csv_file = f'results_{datasets_name}_{dataset_name}_{model_name}.csv'\n",
    "        df = pd.read_csv(csv_file, comment='#')\n",
    "\n",
    "        accuracy_values = df['mean_acc_[' + model_name + ']']\n",
    "        sample_counts = df['id']\n",
    "\n",
    "        # Plot the accuracy over time\n",
    "        plt.plot(sample_counts, accuracy_values, label=model_name)\n",
    "\n",
    "    plt.title(f'Prequential Accuracy Comparison on {dataset_name} ({datasets_name})')\n",
    "    plt.xlabel('Number of samples processed')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'comparison_{datasets_name}_{dataset_name}.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Apply the method for all 4 datasets\n",
    "plot_comparison(artificialDatasets, \"artificial\", [\"AWE\", \"SAMKNN\", \"DWM\", \"ARF\", \"LB\", \"MLP\"], \"WaveformDataset\")\n",
    "plot_comparison(artificialDatasets, \"artificial\", [\"AWE\", \"SAMKNN\", \"DWM\", \"ARF\", \"LB\", \"MLP\"], \"HyperplaneDataset\")\n",
    "plot_comparison(realWorldDatasets, \"real-world\", [\"AWE\", \"SAMKNN\", \"DWM\", \"ARF\", \"LB\", \"MLP\"], \"rialto\")\n",
    "plot_comparison(realWorldDatasets, \"real-world\", [\"AWE\", \"SAMKNN\", \"DWM\", \"ARF\", \"LB\", \"MLP\"], \"spam\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
